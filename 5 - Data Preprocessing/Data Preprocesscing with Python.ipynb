{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing the libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing the dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "0   France  44.0  72000.0        No\n",
       "1    Spain  27.0  48000.0       Yes\n",
       "2  Germany  30.0  54000.0        No\n",
       "3    Spain  38.0  61000.0        No\n",
       "4  Germany  40.0      NaN       Yes"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('Data.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, nan],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', nan, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset.iloc[:, :-1].values # .values to convert to numpy array\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = dataset.iloc[:, -1].values\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Taking care of missing data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country      0\n",
       "Age          1\n",
       "Salary       1\n",
       "Purchased    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing data\n",
    "missing_data = dataset.isnull().sum()\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SimpleImputer in module sklearn.impute._base:\n",
      "\n",
      "class SimpleImputer(_BaseImputer)\n",
      " |  SimpleImputer(*, missing_values=nan, strategy='mean', fill_value=None, copy=True, add_indicator=False, keep_empty_features=False)\n",
      " |\n",
      " |  Univariate imputer for completing missing values with simple strategies.\n",
      " |\n",
      " |  Replace missing values using a descriptive statistic (e.g. mean, median, or\n",
      " |  most frequent) along each column, or using a constant value.\n",
      " |\n",
      " |  Read more in the :ref:`User Guide <impute>`.\n",
      " |\n",
      " |  .. versionadded:: 0.20\n",
      " |     `SimpleImputer` replaces the previous `sklearn.preprocessing.Imputer`\n",
      " |     estimator which is now removed.\n",
      " |\n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  missing_values : int, float, str, np.nan, None or pandas.NA, default=np.nan\n",
      " |      The placeholder for the missing values. All occurrences of\n",
      " |      `missing_values` will be imputed. For pandas' dataframes with\n",
      " |      nullable integer dtypes with missing values, `missing_values`\n",
      " |      can be set to either `np.nan` or `pd.NA`.\n",
      " |\n",
      " |  strategy : str, default='mean'\n",
      " |      The imputation strategy.\n",
      " |\n",
      " |      - If \"mean\", then replace missing values using the mean along\n",
      " |        each column. Can only be used with numeric data.\n",
      " |      - If \"median\", then replace missing values using the median along\n",
      " |        each column. Can only be used with numeric data.\n",
      " |      - If \"most_frequent\", then replace missing using the most frequent\n",
      " |        value along each column. Can be used with strings or numeric data.\n",
      " |        If there is more than one such value, only the smallest is returned.\n",
      " |      - If \"constant\", then replace missing values with fill_value. Can be\n",
      " |        used with strings or numeric data.\n",
      " |\n",
      " |      .. versionadded:: 0.20\n",
      " |         strategy=\"constant\" for fixed value imputation.\n",
      " |\n",
      " |  fill_value : str or numerical value, default=None\n",
      " |      When strategy == \"constant\", `fill_value` is used to replace all\n",
      " |      occurrences of missing_values. For string or object data types,\n",
      " |      `fill_value` must be a string.\n",
      " |      If `None`, `fill_value` will be 0 when imputing numerical\n",
      " |      data and \"missing_value\" for strings or object data types.\n",
      " |\n",
      " |  copy : bool, default=True\n",
      " |      If True, a copy of X will be created. If False, imputation will\n",
      " |      be done in-place whenever possible. Note that, in the following cases,\n",
      " |      a new copy will always be made, even if `copy=False`:\n",
      " |\n",
      " |      - If `X` is not an array of floating values;\n",
      " |      - If `X` is encoded as a CSR matrix;\n",
      " |      - If `add_indicator=True`.\n",
      " |\n",
      " |  add_indicator : bool, default=False\n",
      " |      If True, a :class:`MissingIndicator` transform will stack onto output\n",
      " |      of the imputer's transform. This allows a predictive estimator\n",
      " |      to account for missingness despite imputation. If a feature has no\n",
      " |      missing values at fit/train time, the feature won't appear on\n",
      " |      the missing indicator even if there are missing values at\n",
      " |      transform/test time.\n",
      " |\n",
      " |  keep_empty_features : bool, default=False\n",
      " |      If True, features that consist exclusively of missing values when\n",
      " |      `fit` is called are returned in results when `transform` is called.\n",
      " |      The imputed value is always `0` except when `strategy=\"constant\"`\n",
      " |      in which case `fill_value` will be used instead.\n",
      " |\n",
      " |      .. versionadded:: 1.2\n",
      " |\n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  statistics_ : array of shape (n_features,)\n",
      " |      The imputation fill value for each feature.\n",
      " |      Computing statistics can result in `np.nan` values.\n",
      " |      During :meth:`transform`, features corresponding to `np.nan`\n",
      " |      statistics will be discarded.\n",
      " |\n",
      " |  indicator_ : :class:`~sklearn.impute.MissingIndicator`\n",
      " |      Indicator used to add binary indicators for missing values.\n",
      " |      `None` if `add_indicator=False`.\n",
      " |\n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |\n",
      " |      .. versionadded:: 0.24\n",
      " |\n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |\n",
      " |      .. versionadded:: 1.0\n",
      " |\n",
      " |  See Also\n",
      " |  --------\n",
      " |  IterativeImputer : Multivariate imputer that estimates values to impute for\n",
      " |      each feature with missing values from all the others.\n",
      " |  KNNImputer : Multivariate imputer that estimates missing features using\n",
      " |      nearest samples.\n",
      " |\n",
      " |  Notes\n",
      " |  -----\n",
      " |  Columns which only contained missing values at :meth:`fit` are discarded\n",
      " |  upon :meth:`transform` if strategy is not `\"constant\"`.\n",
      " |\n",
      " |  In a prediction context, simple imputation usually performs poorly when\n",
      " |  associated with a weak learner. However, with a powerful learner, it can\n",
      " |  lead to as good or better performance than complex imputation such as\n",
      " |  :class:`~sklearn.impute.IterativeImputer` or :class:`~sklearn.impute.KNNImputer`.\n",
      " |\n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.impute import SimpleImputer\n",
      " |  >>> imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
      " |  >>> imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])\n",
      " |  SimpleImputer()\n",
      " |  >>> X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]\n",
      " |  >>> print(imp_mean.transform(X))\n",
      " |  [[ 7.   2.   3. ]\n",
      " |   [ 4.   3.5  6. ]\n",
      " |   [10.   3.5  9. ]]\n",
      " |\n",
      " |  For a more detailed example see\n",
      " |  :ref:`sphx_glr_auto_examples_impute_plot_missing_values.py`.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      SimpleImputer\n",
      " |      _BaseImputer\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.utils._set_output._SetOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, *, missing_values=nan, strategy='mean', fill_value=None, copy=True, add_indicator=False, keep_empty_features=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  fit(self, X, y=None)\n",
      " |      Fit the imputer on `X`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          Input data, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |\n",
      " |      y : Ignored\n",
      " |          Not used, present here for API consistency by convention.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |\n",
      " |  get_feature_names_out(self, input_features=None)\n",
      " |      Get output feature names for transformation.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_features : array-like of str or None, default=None\n",
      " |          Input features.\n",
      " |\n",
      " |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      " |            used as feature names in. If `feature_names_in_` is not defined,\n",
      " |            then the following input feature names are generated:\n",
      " |            `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      " |          - If `input_features` is an array-like, then `input_features` must\n",
      " |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names_out : ndarray of str objects\n",
      " |          Transformed feature names.\n",
      " |\n",
      " |  inverse_transform(self, X)\n",
      " |      Convert the data back to the original representation.\n",
      " |\n",
      " |      Inverts the `transform` operation performed on an array.\n",
      " |      This operation can only be performed after :class:`SimpleImputer` is\n",
      " |      instantiated with `add_indicator=True`.\n",
      " |\n",
      " |      Note that `inverse_transform` can only invert the transform in\n",
      " |      features that have binary indicators for missing values. If a feature\n",
      " |      has no missing values at `fit` time, the feature won't have a binary\n",
      " |      indicator, and the imputation done at `transform` time won't be\n",
      " |      inverted.\n",
      " |\n",
      " |      .. versionadded:: 0.24\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape                 (n_samples, n_features + n_features_missing_indicator)\n",
      " |          The imputed data to be reverted to original data. It has to be\n",
      " |          an augmented array of imputed data and the missing indicator mask.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_original : ndarray of shape (n_samples, n_features)\n",
      " |          The original `X` with missing values as it was prior\n",
      " |          to imputation.\n",
      " |\n",
      " |  transform(self, X)\n",
      " |      Impute all missing values in `X`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input data to complete.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_imputed : {ndarray, sparse matrix} of shape                 (n_samples, n_features_out)\n",
      " |          `X` with imputed values.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |\n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |\n",
      " |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      " |      and returns a transformed version of `X`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input samples.\n",
      " |\n",
      " |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      " |          Target values (None for unsupervised transformations).\n",
      " |\n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |\n",
      " |  set_output(self, *, transform=None)\n",
      " |      Set output container.\n",
      " |\n",
      " |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      " |      for an example on how to use the API.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      transform : {\"default\", \"pandas\"}, default=None\n",
      " |          Configure output of `transform` and `fit_transform`.\n",
      " |\n",
      " |          - `\"default\"`: Default output format of a transformer\n",
      " |          - `\"pandas\"`: DataFrame output\n",
      " |          - `\"polars\"`: Polars output\n",
      " |          - `None`: Transform configuration is unchanged\n",
      " |\n",
      " |          .. versionadded:: 1.4\n",
      " |              `\"polars\"` option was added.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |\n",
      " |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs) from builtins.type\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |\n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |\n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |\n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |\n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __setstate__(self, state)\n",
      " |\n",
      " |  __sklearn_clone__(self)\n",
      " |\n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |\n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |\n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |\n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |\n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "help(SimpleImputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean') # `imputer`is object of the `SimpleImputer` to replace missing values with the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`imputer` is just an object, we haven't connect anything yet to our matrix of features, so the next step is indeed to apply this `imputer` object on the matrix of features. Then we will call the `fit()` method, the `fit()` method will exactly connect this `imputer` to the matrix of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SimpleImputer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SimpleImputer()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "SimpleImputer()"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer.fit(X[:, 1:3]) # fit the imputer to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:, 1:3] = imputer.transform(X[:, 1:3]) # replace the missing values with the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, 63777.77777777778],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', 38.77777777777778, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Encoding categorical data**\n",
    "\n",
    "This dataset contains a column with categories: France, Spain, or Germany. When working with machine learning models, it can be challenging for the model to compute correlations between these categorical columns. To address this, we need to convert these strings into numerical values.\n",
    "\n",
    "One approach would be to encode France as 0, Spain as 1, and Germany as 2. This way, the model would understand that France is represented by 0, Spain by 1, and Germany by 2. However, this encoding may lead to misinterpreted correlations because the model might assume a numerical relationship between these countries.\n",
    "\n",
    "A better approach is to use one hot encoding. One hot encoding involves creating separate columns for each category (in this case, we'll have three columns for the three countries). Each column represents a binary vector. For example, France would be represented by the vector `[1 0 0]`, Spain by `[0 1 0]`, and Germany by `[0 0 1]`. In this way, we only have zeros and ones, and each country is represented by a unique combination of these binary values.\n",
    "\n",
    "Additionally, there is another column called \"purchased\" that contains non-numerical labels. We also need to convert these labels into zeros and ones to make them compatible with the machine learning model.\n",
    "\n",
    "+ Encoding the Independent Variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ColumnTransformer in module sklearn.compose._column_transformer:\n",
      "\n",
      "class ColumnTransformer(sklearn.base.TransformerMixin, sklearn.utils.metaestimators._BaseComposition)\n",
      " |  ColumnTransformer(transformers, *, remainder='drop', sparse_threshold=0.3, n_jobs=None, transformer_weights=None, verbose=False, verbose_feature_names_out=True)\n",
      " |\n",
      " |  Applies transformers to columns of an array or pandas DataFrame.\n",
      " |\n",
      " |  This estimator allows different columns or column subsets of the input\n",
      " |  to be transformed separately and the features generated by each transformer\n",
      " |  will be concatenated to form a single feature space.\n",
      " |  This is useful for heterogeneous or columnar data, to combine several\n",
      " |  feature extraction mechanisms or transformations into a single transformer.\n",
      " |\n",
      " |  Read more in the :ref:`User Guide <column_transformer>`.\n",
      " |\n",
      " |  .. versionadded:: 0.20\n",
      " |\n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  transformers : list of tuples\n",
      " |      List of (name, transformer, columns) tuples specifying the\n",
      " |      transformer objects to be applied to subsets of the data.\n",
      " |\n",
      " |      name : str\n",
      " |          Like in Pipeline and FeatureUnion, this allows the transformer and\n",
      " |          its parameters to be set using ``set_params`` and searched in grid\n",
      " |          search.\n",
      " |      transformer : {'drop', 'passthrough'} or estimator\n",
      " |          Estimator must support :term:`fit` and :term:`transform`.\n",
      " |          Special-cased strings 'drop' and 'passthrough' are accepted as\n",
      " |          well, to indicate to drop the columns or to pass them through\n",
      " |          untransformed, respectively.\n",
      " |      columns :  str, array-like of str, int, array-like of int,                 array-like of bool, slice or callable\n",
      " |          Indexes the data on its second axis. Integers are interpreted as\n",
      " |          positional columns, while strings can reference DataFrame columns\n",
      " |          by name.  A scalar string or int should be used where\n",
      " |          ``transformer`` expects X to be a 1d array-like (vector),\n",
      " |          otherwise a 2d array will be passed to the transformer.\n",
      " |          A callable is passed the input data `X` and can return any of the\n",
      " |          above. To select multiple columns by name or dtype, you can use\n",
      " |          :obj:`make_column_selector`.\n",
      " |\n",
      " |  remainder : {'drop', 'passthrough'} or estimator, default='drop'\n",
      " |      By default, only the specified columns in `transformers` are\n",
      " |      transformed and combined in the output, and the non-specified\n",
      " |      columns are dropped. (default of ``'drop'``).\n",
      " |      By specifying ``remainder='passthrough'``, all remaining columns that\n",
      " |      were not specified in `transformers`, but present in the data passed\n",
      " |      to `fit` will be automatically passed through. This subset of columns\n",
      " |      is concatenated with the output of the transformers. For dataframes,\n",
      " |      extra columns not seen during `fit` will be excluded from the output\n",
      " |      of `transform`.\n",
      " |      By setting ``remainder`` to be an estimator, the remaining\n",
      " |      non-specified columns will use the ``remainder`` estimator. The\n",
      " |      estimator must support :term:`fit` and :term:`transform`.\n",
      " |      Note that using this feature requires that the DataFrame columns\n",
      " |      input at :term:`fit` and :term:`transform` have identical order.\n",
      " |\n",
      " |  sparse_threshold : float, default=0.3\n",
      " |      If the output of the different transformers contains sparse matrices,\n",
      " |      these will be stacked as a sparse matrix if the overall density is\n",
      " |      lower than this value. Use ``sparse_threshold=0`` to always return\n",
      " |      dense.  When the transformed output consists of all dense data, the\n",
      " |      stacked result will be dense, and this keyword will be ignored.\n",
      " |\n",
      " |  n_jobs : int, default=None\n",
      " |      Number of jobs to run in parallel.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |\n",
      " |  transformer_weights : dict, default=None\n",
      " |      Multiplicative weights for features per transformer. The output of the\n",
      " |      transformer is multiplied by these weights. Keys are transformer names,\n",
      " |      values the weights.\n",
      " |\n",
      " |  verbose : bool, default=False\n",
      " |      If True, the time elapsed while fitting each transformer will be\n",
      " |      printed as it is completed.\n",
      " |\n",
      " |  verbose_feature_names_out : bool, default=True\n",
      " |      If True, :meth:`ColumnTransformer.get_feature_names_out` will prefix\n",
      " |      all feature names with the name of the transformer that generated that\n",
      " |      feature.\n",
      " |      If False, :meth:`ColumnTransformer.get_feature_names_out` will not\n",
      " |      prefix any feature names and will error if feature names are not\n",
      " |      unique.\n",
      " |\n",
      " |      .. versionadded:: 1.0\n",
      " |\n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  transformers_ : list\n",
      " |      The collection of fitted transformers as tuples of (name,\n",
      " |      fitted_transformer, column). `fitted_transformer` can be an estimator,\n",
      " |      or `'drop'`; `'passthrough'` is replaced with an equivalent\n",
      " |      :class:`~sklearn.preprocessing.FunctionTransformer`. In case there were\n",
      " |      no columns selected, this will be the unfitted transformer. If there\n",
      " |      are remaining columns, the final element is a tuple of the form:\n",
      " |      ('remainder', transformer, remaining_columns) corresponding to the\n",
      " |      ``remainder`` parameter. If there are remaining columns, then\n",
      " |      ``len(transformers_)==len(transformers)+1``, otherwise\n",
      " |      ``len(transformers_)==len(transformers)``.\n",
      " |\n",
      " |  named_transformers_ : :class:`~sklearn.utils.Bunch`\n",
      " |      Read-only attribute to access any transformer by given name.\n",
      " |      Keys are transformer names and values are the fitted transformer\n",
      " |      objects.\n",
      " |\n",
      " |  sparse_output_ : bool\n",
      " |      Boolean flag indicating whether the output of ``transform`` is a\n",
      " |      sparse matrix or a dense numpy array, which depends on the output\n",
      " |      of the individual transformers and the `sparse_threshold` keyword.\n",
      " |\n",
      " |  output_indices_ : dict\n",
      " |      A dictionary from each transformer name to a slice, where the slice\n",
      " |      corresponds to indices in the transformed output. This is useful to\n",
      " |      inspect which transformer is responsible for which transformed\n",
      " |      feature(s).\n",
      " |\n",
      " |      .. versionadded:: 1.0\n",
      " |\n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`. Only defined if the\n",
      " |      underlying transformers expose such an attribute when fit.\n",
      " |\n",
      " |      .. versionadded:: 0.24\n",
      " |\n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |\n",
      " |      .. versionadded:: 1.0\n",
      " |\n",
      " |  See Also\n",
      " |  --------\n",
      " |  make_column_transformer : Convenience function for\n",
      " |      combining the outputs of multiple transformer objects applied to\n",
      " |      column subsets of the original feature space.\n",
      " |  make_column_selector : Convenience function for selecting\n",
      " |      columns based on datatype or the columns name with a regex pattern.\n",
      " |\n",
      " |  Notes\n",
      " |  -----\n",
      " |  The order of the columns in the transformed feature matrix follows the\n",
      " |  order of how the columns are specified in the `transformers` list.\n",
      " |  Columns of the original feature matrix that are not specified are\n",
      " |  dropped from the resulting transformed feature matrix, unless specified\n",
      " |  in the `passthrough` keyword. Those columns specified with `passthrough`\n",
      " |  are added at the right to the output of the transformers.\n",
      " |\n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.compose import ColumnTransformer\n",
      " |  >>> from sklearn.preprocessing import Normalizer\n",
      " |  >>> ct = ColumnTransformer(\n",
      " |  ...     [(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n",
      " |  ...      (\"norm2\", Normalizer(norm='l1'), slice(2, 4))])\n",
      " |  >>> X = np.array([[0., 1., 2., 2.],\n",
      " |  ...               [1., 1., 0., 1.]])\n",
      " |  >>> # Normalizer scales each row of X to unit norm. A separate scaling\n",
      " |  >>> # is applied for the two first and two last elements of each\n",
      " |  >>> # row independently.\n",
      " |  >>> ct.fit_transform(X)\n",
      " |  array([[0. , 1. , 0.5, 0.5],\n",
      " |         [0.5, 0.5, 0. , 1. ]])\n",
      " |\n",
      " |  :class:`ColumnTransformer` can be configured with a transformer that requires\n",
      " |  a 1d array by setting the column to a string:\n",
      " |\n",
      " |  >>> from sklearn.feature_extraction.text import CountVectorizer\n",
      " |  >>> from sklearn.preprocessing import MinMaxScaler\n",
      " |  >>> import pandas as pd   # doctest: +SKIP\n",
      " |  >>> X = pd.DataFrame({\n",
      " |  ...     \"documents\": [\"First item\", \"second one here\", \"Is this the last?\"],\n",
      " |  ...     \"width\": [3, 4, 5],\n",
      " |  ... })  # doctest: +SKIP\n",
      " |  >>> # \"documents\" is a string which configures ColumnTransformer to\n",
      " |  >>> # pass the documents column as a 1d array to the CountVectorizer\n",
      " |  >>> ct = ColumnTransformer(\n",
      " |  ...     [(\"text_preprocess\", CountVectorizer(), \"documents\"),\n",
      " |  ...      (\"num_preprocess\", MinMaxScaler(), [\"width\"])])\n",
      " |  >>> X_trans = ct.fit_transform(X)  # doctest: +SKIP\n",
      " |\n",
      " |  For a more detailed example of usage, see\n",
      " |  :ref:`sphx_glr_auto_examples_compose_plot_column_transformer_mixed_types.py`.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      ColumnTransformer\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.utils._set_output._SetOutputMixin\n",
      " |      sklearn.utils.metaestimators._BaseComposition\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, transformers, *, remainder='drop', sparse_threshold=0.3, n_jobs=None, transformer_weights=None, verbose=False, verbose_feature_names_out=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  fit(self, X, y=None, **params)\n",
      " |      Fit all transformers using X.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, dataframe} of shape (n_samples, n_features)\n",
      " |          Input data, of which specified subsets are used to fit the\n",
      " |          transformers.\n",
      " |\n",
      " |      y : array-like of shape (n_samples,...), default=None\n",
      " |          Targets for supervised learning.\n",
      " |\n",
      " |      **params : dict, default=None\n",
      " |          Parameters to be passed to the underlying transformers' ``fit`` and\n",
      " |          ``transform`` methods.\n",
      " |\n",
      " |          You can only pass this if metadata routing is enabled, which you\n",
      " |          can enable using ``sklearn.set_config(enable_metadata_routing=True)``.\n",
      " |\n",
      " |          .. versionadded:: 1.4\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : ColumnTransformer\n",
      " |          This estimator.\n",
      " |\n",
      " |  fit_transform(self, X, y=None, **params)\n",
      " |      Fit all transformers, transform the data and concatenate results.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, dataframe} of shape (n_samples, n_features)\n",
      " |          Input data, of which specified subsets are used to fit the\n",
      " |          transformers.\n",
      " |\n",
      " |      y : array-like of shape (n_samples,), default=None\n",
      " |          Targets for supervised learning.\n",
      " |\n",
      " |      **params : dict, default=None\n",
      " |          Parameters to be passed to the underlying transformers' ``fit`` and\n",
      " |          ``transform`` methods.\n",
      " |\n",
      " |          You can only pass this if metadata routing is enabled, which you\n",
      " |          can enable using ``sklearn.set_config(enable_metadata_routing=True)``.\n",
      " |\n",
      " |          .. versionadded:: 1.4\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_t : {array-like, sparse matrix} of                 shape (n_samples, sum_n_components)\n",
      " |          Horizontally stacked results of transformers. sum_n_components is the\n",
      " |          sum of n_components (output dimension) over transformers. If\n",
      " |          any result is a sparse matrix, everything will be converted to\n",
      " |          sparse matrices.\n",
      " |\n",
      " |  get_feature_names_out(self, input_features=None)\n",
      " |      Get output feature names for transformation.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_features : array-like of str or None, default=None\n",
      " |          Input features.\n",
      " |\n",
      " |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      " |            used as feature names in. If `feature_names_in_` is not defined,\n",
      " |            then the following input feature names are generated:\n",
      " |            `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      " |          - If `input_features` is an array-like, then `input_features` must\n",
      " |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names_out : ndarray of str objects\n",
      " |          Transformed feature names.\n",
      " |\n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |\n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |\n",
      " |      .. versionadded:: 1.4\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRouter\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
      " |          routing information.\n",
      " |\n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |\n",
      " |      Returns the parameters given in the constructor as well as the\n",
      " |      estimators contained within the `transformers` of the\n",
      " |      `ColumnTransformer`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |\n",
      " |  set_output(self, *, transform=None)\n",
      " |      Set the output container when `\"transform\"` and `\"fit_transform\"` are called.\n",
      " |\n",
      " |      Calling `set_output` will set the output of all estimators in `transformers`\n",
      " |      and `transformers_`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      transform : {\"default\", \"pandas\"}, default=None\n",
      " |          Configure output of `transform` and `fit_transform`.\n",
      " |\n",
      " |          - `\"default\"`: Default output format of a transformer\n",
      " |          - `\"pandas\"`: DataFrame output\n",
      " |          - `\"polars\"`: Polars output\n",
      " |          - `None`: Transform configuration is unchanged\n",
      " |\n",
      " |          .. versionadded:: 1.4\n",
      " |              `\"polars\"` option was added.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |\n",
      " |  set_params(self, **kwargs)\n",
      " |      Set the parameters of this estimator.\n",
      " |\n",
      " |      Valid parameter keys can be listed with ``get_params()``. Note that you\n",
      " |      can directly set the parameters of the estimators contained in\n",
      " |      `transformers` of `ColumnTransformer`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs : dict\n",
      " |          Estimator parameters.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : ColumnTransformer\n",
      " |          This estimator.\n",
      " |\n",
      " |  transform(self, X, **params)\n",
      " |      Transform X separately by each transformer, concatenate results.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, dataframe} of shape (n_samples, n_features)\n",
      " |          The data to be transformed by subset.\n",
      " |\n",
      " |      **params : dict, default=None\n",
      " |          Parameters to be passed to the underlying transformers' ``transform``\n",
      " |          method.\n",
      " |\n",
      " |          You can only pass this if metadata routing is enabled, which you\n",
      " |          can enable using ``sklearn.set_config(enable_metadata_routing=True)``.\n",
      " |\n",
      " |          .. versionadded:: 1.4\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_t : {array-like, sparse matrix} of                 shape (n_samples, sum_n_components)\n",
      " |          Horizontally stacked results of transformers. sum_n_components is the\n",
      " |          sum of n_components (output dimension) over transformers. If\n",
      " |          any result is a sparse matrix, everything will be converted to\n",
      " |          sparse matrices.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  named_transformers_\n",
      " |      Access the fitted transformer by name.\n",
      " |\n",
      " |      Read-only attribute to access any transformer by given name.\n",
      " |      Keys are transformer names and values are the fitted transformer\n",
      " |      objects.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |\n",
      " |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs) from abc.ABCMeta\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |\n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |\n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |\n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |\n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __setstate__(self, state)\n",
      " |\n",
      " |  __sklearn_clone__(self)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ColumnTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class OneHotEncoder in module sklearn.preprocessing._encoders:\n",
      "\n",
      "class OneHotEncoder(_BaseEncoder)\n",
      " |  OneHotEncoder(*, categories='auto', drop=None, sparse_output=True, dtype=<class 'numpy.float64'>, handle_unknown='error', min_frequency=None, max_categories=None, feature_name_combiner='concat')\n",
      " |\n",
      " |  Encode categorical features as a one-hot numeric array.\n",
      " |\n",
      " |  The input to this transformer should be an array-like of integers or\n",
      " |  strings, denoting the values taken on by categorical (discrete) features.\n",
      " |  The features are encoded using a one-hot (aka 'one-of-K' or 'dummy')\n",
      " |  encoding scheme. This creates a binary column for each category and\n",
      " |  returns a sparse matrix or dense array (depending on the ``sparse_output``\n",
      " |  parameter).\n",
      " |\n",
      " |  By default, the encoder derives the categories based on the unique values\n",
      " |  in each feature. Alternatively, you can also specify the `categories`\n",
      " |  manually.\n",
      " |\n",
      " |  This encoding is needed for feeding categorical data to many scikit-learn\n",
      " |  estimators, notably linear models and SVMs with the standard kernels.\n",
      " |\n",
      " |  Note: a one-hot encoding of y labels should use a LabelBinarizer\n",
      " |  instead.\n",
      " |\n",
      " |  Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
      " |  For a comparison of different encoders, refer to:\n",
      " |  :ref:`sphx_glr_auto_examples_preprocessing_plot_target_encoder.py`.\n",
      " |\n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  categories : 'auto' or a list of array-like, default='auto'\n",
      " |      Categories (unique values) per feature:\n",
      " |\n",
      " |      - 'auto' : Determine categories automatically from the training data.\n",
      " |      - list : ``categories[i]`` holds the categories expected in the ith\n",
      " |        column. The passed categories should not mix strings and numeric\n",
      " |        values within a single feature, and should be sorted in case of\n",
      " |        numeric values.\n",
      " |\n",
      " |      The used categories can be found in the ``categories_`` attribute.\n",
      " |\n",
      " |      .. versionadded:: 0.20\n",
      " |\n",
      " |  drop : {'first', 'if_binary'} or an array-like of shape (n_features,),             default=None\n",
      " |      Specifies a methodology to use to drop one of the categories per\n",
      " |      feature. This is useful in situations where perfectly collinear\n",
      " |      features cause problems, such as when feeding the resulting data\n",
      " |      into an unregularized linear regression model.\n",
      " |\n",
      " |      However, dropping one category breaks the symmetry of the original\n",
      " |      representation and can therefore induce a bias in downstream models,\n",
      " |      for instance for penalized linear classification or regression models.\n",
      " |\n",
      " |      - None : retain all features (the default).\n",
      " |      - 'first' : drop the first category in each feature. If only one\n",
      " |        category is present, the feature will be dropped entirely.\n",
      " |      - 'if_binary' : drop the first category in each feature with two\n",
      " |        categories. Features with 1 or more than 2 categories are\n",
      " |        left intact.\n",
      " |      - array : ``drop[i]`` is the category in feature ``X[:, i]`` that\n",
      " |        should be dropped.\n",
      " |\n",
      " |      When `max_categories` or `min_frequency` is configured to group\n",
      " |      infrequent categories, the dropping behavior is handled after the\n",
      " |      grouping.\n",
      " |\n",
      " |      .. versionadded:: 0.21\n",
      " |         The parameter `drop` was added in 0.21.\n",
      " |\n",
      " |      .. versionchanged:: 0.23\n",
      " |         The option `drop='if_binary'` was added in 0.23.\n",
      " |\n",
      " |      .. versionchanged:: 1.1\n",
      " |          Support for dropping infrequent categories.\n",
      " |\n",
      " |  sparse_output : bool, default=True\n",
      " |      When ``True``, it returns a :class:`scipy.sparse.csr_matrix`,\n",
      " |      i.e. a sparse matrix in \"Compressed Sparse Row\" (CSR) format.\n",
      " |\n",
      " |      .. versionadded:: 1.2\n",
      " |         `sparse` was renamed to `sparse_output`\n",
      " |\n",
      " |  dtype : number type, default=np.float64\n",
      " |      Desired dtype of output.\n",
      " |\n",
      " |  handle_unknown : {'error', 'ignore', 'infrequent_if_exist'},                      default='error'\n",
      " |      Specifies the way unknown categories are handled during :meth:`transform`.\n",
      " |\n",
      " |      - 'error' : Raise an error if an unknown category is present during transform.\n",
      " |      - 'ignore' : When an unknown category is encountered during\n",
      " |        transform, the resulting one-hot encoded columns for this feature\n",
      " |        will be all zeros. In the inverse transform, an unknown category\n",
      " |        will be denoted as None.\n",
      " |      - 'infrequent_if_exist' : When an unknown category is encountered\n",
      " |        during transform, the resulting one-hot encoded columns for this\n",
      " |        feature will map to the infrequent category if it exists. The\n",
      " |        infrequent category will be mapped to the last position in the\n",
      " |        encoding. During inverse transform, an unknown category will be\n",
      " |        mapped to the category denoted `'infrequent'` if it exists. If the\n",
      " |        `'infrequent'` category does not exist, then :meth:`transform` and\n",
      " |        :meth:`inverse_transform` will handle an unknown category as with\n",
      " |        `handle_unknown='ignore'`. Infrequent categories exist based on\n",
      " |        `min_frequency` and `max_categories`. Read more in the\n",
      " |        :ref:`User Guide <encoder_infrequent_categories>`.\n",
      " |\n",
      " |      .. versionchanged:: 1.1\n",
      " |          `'infrequent_if_exist'` was added to automatically handle unknown\n",
      " |          categories and infrequent categories.\n",
      " |\n",
      " |  min_frequency : int or float, default=None\n",
      " |      Specifies the minimum frequency below which a category will be\n",
      " |      considered infrequent.\n",
      " |\n",
      " |      - If `int`, categories with a smaller cardinality will be considered\n",
      " |        infrequent.\n",
      " |\n",
      " |      - If `float`, categories with a smaller cardinality than\n",
      " |        `min_frequency * n_samples`  will be considered infrequent.\n",
      " |\n",
      " |      .. versionadded:: 1.1\n",
      " |          Read more in the :ref:`User Guide <encoder_infrequent_categories>`.\n",
      " |\n",
      " |  max_categories : int, default=None\n",
      " |      Specifies an upper limit to the number of output features for each input\n",
      " |      feature when considering infrequent categories. If there are infrequent\n",
      " |      categories, `max_categories` includes the category representing the\n",
      " |      infrequent categories along with the frequent categories. If `None`,\n",
      " |      there is no limit to the number of output features.\n",
      " |\n",
      " |      .. versionadded:: 1.1\n",
      " |          Read more in the :ref:`User Guide <encoder_infrequent_categories>`.\n",
      " |\n",
      " |  feature_name_combiner : \"concat\" or callable, default=\"concat\"\n",
      " |      Callable with signature `def callable(input_feature, category)` that returns a\n",
      " |      string. This is used to create feature names to be returned by\n",
      " |      :meth:`get_feature_names_out`.\n",
      " |\n",
      " |      `\"concat\"` concatenates encoded feature name and category with\n",
      " |      `feature + \"_\" + str(category)`.E.g. feature X with values 1, 6, 7 create\n",
      " |      feature names `X_1, X_6, X_7`.\n",
      " |\n",
      " |      .. versionadded:: 1.3\n",
      " |\n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  categories_ : list of arrays\n",
      " |      The categories of each feature determined during fitting\n",
      " |      (in order of the features in X and corresponding with the output\n",
      " |      of ``transform``). This includes the category specified in ``drop``\n",
      " |      (if any).\n",
      " |\n",
      " |  drop_idx_ : array of shape (n_features,)\n",
      " |      - ``drop_idx_[i]`` is the index in ``categories_[i]`` of the category\n",
      " |        to be dropped for each feature.\n",
      " |      - ``drop_idx_[i] = None`` if no category is to be dropped from the\n",
      " |        feature with index ``i``, e.g. when `drop='if_binary'` and the\n",
      " |        feature isn't binary.\n",
      " |      - ``drop_idx_ = None`` if all the transformed features will be\n",
      " |        retained.\n",
      " |\n",
      " |      If infrequent categories are enabled by setting `min_frequency` or\n",
      " |      `max_categories` to a non-default value and `drop_idx[i]` corresponds\n",
      " |      to a infrequent category, then the entire infrequent category is\n",
      " |      dropped.\n",
      " |\n",
      " |      .. versionchanged:: 0.23\n",
      " |         Added the possibility to contain `None` values.\n",
      " |\n",
      " |  infrequent_categories_ : list of ndarray\n",
      " |      Defined only if infrequent categories are enabled by setting\n",
      " |      `min_frequency` or `max_categories` to a non-default value.\n",
      " |      `infrequent_categories_[i]` are the infrequent categories for feature\n",
      " |      `i`. If the feature `i` has no infrequent categories\n",
      " |      `infrequent_categories_[i]` is None.\n",
      " |\n",
      " |      .. versionadded:: 1.1\n",
      " |\n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |\n",
      " |      .. versionadded:: 1.0\n",
      " |\n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |\n",
      " |      .. versionadded:: 1.0\n",
      " |\n",
      " |  feature_name_combiner : callable or None\n",
      " |      Callable with signature `def callable(input_feature, category)` that returns a\n",
      " |      string. This is used to create feature names to be returned by\n",
      " |      :meth:`get_feature_names_out`.\n",
      " |\n",
      " |      .. versionadded:: 1.3\n",
      " |\n",
      " |  See Also\n",
      " |  --------\n",
      " |  OrdinalEncoder : Performs an ordinal (integer)\n",
      " |    encoding of the categorical features.\n",
      " |  TargetEncoder : Encodes categorical features using the target.\n",
      " |  sklearn.feature_extraction.DictVectorizer : Performs a one-hot encoding of\n",
      " |    dictionary items (also handles string-valued features).\n",
      " |  sklearn.feature_extraction.FeatureHasher : Performs an approximate one-hot\n",
      " |    encoding of dictionary items or strings.\n",
      " |  LabelBinarizer : Binarizes labels in a one-vs-all\n",
      " |    fashion.\n",
      " |  MultiLabelBinarizer : Transforms between iterable of\n",
      " |    iterables and a multilabel format, e.g. a (samples x classes) binary\n",
      " |    matrix indicating the presence of a class label.\n",
      " |\n",
      " |  Examples\n",
      " |  --------\n",
      " |  Given a dataset with two features, we let the encoder find the unique\n",
      " |  values per feature and transform the data to a binary one-hot encoding.\n",
      " |\n",
      " |  >>> from sklearn.preprocessing import OneHotEncoder\n",
      " |\n",
      " |  One can discard categories not seen during `fit`:\n",
      " |\n",
      " |  >>> enc = OneHotEncoder(handle_unknown='ignore')\n",
      " |  >>> X = [['Male', 1], ['Female', 3], ['Female', 2]]\n",
      " |  >>> enc.fit(X)\n",
      " |  OneHotEncoder(handle_unknown='ignore')\n",
      " |  >>> enc.categories_\n",
      " |  [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n",
      " |  >>> enc.transform([['Female', 1], ['Male', 4]]).toarray()\n",
      " |  array([[1., 0., 1., 0., 0.],\n",
      " |         [0., 1., 0., 0., 0.]])\n",
      " |  >>> enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])\n",
      " |  array([['Male', 1],\n",
      " |         [None, 2]], dtype=object)\n",
      " |  >>> enc.get_feature_names_out(['gender', 'group'])\n",
      " |  array(['gender_Female', 'gender_Male', 'group_1', 'group_2', 'group_3'], ...)\n",
      " |\n",
      " |  One can always drop the first column for each feature:\n",
      " |\n",
      " |  >>> drop_enc = OneHotEncoder(drop='first').fit(X)\n",
      " |  >>> drop_enc.categories_\n",
      " |  [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n",
      " |  >>> drop_enc.transform([['Female', 1], ['Male', 2]]).toarray()\n",
      " |  array([[0., 0., 0.],\n",
      " |         [1., 1., 0.]])\n",
      " |\n",
      " |  Or drop a column for feature only having 2 categories:\n",
      " |\n",
      " |  >>> drop_binary_enc = OneHotEncoder(drop='if_binary').fit(X)\n",
      " |  >>> drop_binary_enc.transform([['Female', 1], ['Male', 2]]).toarray()\n",
      " |  array([[0., 1., 0., 0.],\n",
      " |         [1., 0., 1., 0.]])\n",
      " |\n",
      " |  One can change the way feature names are created.\n",
      " |\n",
      " |  >>> def custom_combiner(feature, category):\n",
      " |  ...     return str(feature) + \"_\" + type(category).__name__ + \"_\" + str(category)\n",
      " |  >>> custom_fnames_enc = OneHotEncoder(feature_name_combiner=custom_combiner).fit(X)\n",
      " |  >>> custom_fnames_enc.get_feature_names_out()\n",
      " |  array(['x0_str_Female', 'x0_str_Male', 'x1_int_1', 'x1_int_2', 'x1_int_3'],\n",
      " |        dtype=object)\n",
      " |\n",
      " |  Infrequent categories are enabled by setting `max_categories` or `min_frequency`.\n",
      " |\n",
      " |  >>> import numpy as np\n",
      " |  >>> X = np.array([[\"a\"] * 5 + [\"b\"] * 20 + [\"c\"] * 10 + [\"d\"] * 3], dtype=object).T\n",
      " |  >>> ohe = OneHotEncoder(max_categories=3, sparse_output=False).fit(X)\n",
      " |  >>> ohe.infrequent_categories_\n",
      " |  [array(['a', 'd'], dtype=object)]\n",
      " |  >>> ohe.transform([[\"a\"], [\"b\"]])\n",
      " |  array([[0., 0., 1.],\n",
      " |         [1., 0., 0.]])\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      OneHotEncoder\n",
      " |      _BaseEncoder\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.utils._set_output._SetOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, *, categories='auto', drop=None, sparse_output=True, dtype=<class 'numpy.float64'>, handle_unknown='error', min_frequency=None, max_categories=None, feature_name_combiner='concat')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  fit(self, X, y=None)\n",
      " |      Fit OneHotEncoder to X.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data to determine the categories of each feature.\n",
      " |\n",
      " |      y : None\n",
      " |          Ignored. This parameter exists only for compatibility with\n",
      " |          :class:`~sklearn.pipeline.Pipeline`.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted encoder.\n",
      " |\n",
      " |  get_feature_names_out(self, input_features=None)\n",
      " |      Get output feature names for transformation.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_features : array-like of str or None, default=None\n",
      " |          Input features.\n",
      " |\n",
      " |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      " |            used as feature names in. If `feature_names_in_` is not defined,\n",
      " |            then the following input feature names are generated:\n",
      " |            `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      " |          - If `input_features` is an array-like, then `input_features` must\n",
      " |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names_out : ndarray of str objects\n",
      " |          Transformed feature names.\n",
      " |\n",
      " |  inverse_transform(self, X)\n",
      " |      Convert the data back to the original representation.\n",
      " |\n",
      " |      When unknown categories are encountered (all zeros in the\n",
      " |      one-hot encoding), ``None`` is used to represent this category. If the\n",
      " |      feature with the unknown category has a dropped category, the dropped\n",
      " |      category will be its inverse.\n",
      " |\n",
      " |      For a given input feature, if there is an infrequent category,\n",
      " |      'infrequent_sklearn' will be used to represent the infrequent category.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape                 (n_samples, n_encoded_features)\n",
      " |          The transformed data.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : ndarray of shape (n_samples, n_features)\n",
      " |          Inverse transformed array.\n",
      " |\n",
      " |  transform(self, X)\n",
      " |      Transform X using one-hot encoding.\n",
      " |\n",
      " |      If `sparse_output=True` (default), it returns an instance of\n",
      " |      :class:`scipy.sparse._csr.csr_matrix` (CSR format).\n",
      " |\n",
      " |      If there are infrequent categories for a feature, set by specifying\n",
      " |      `max_categories` or `min_frequency`, the infrequent categories are\n",
      " |      grouped into a single category.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data to encode.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_out : {ndarray, sparse matrix} of shape                 (n_samples, n_encoded_features)\n",
      " |          Transformed input. If `sparse_output=True`, a sparse matrix will be\n",
      " |          returned.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from _BaseEncoder:\n",
      " |\n",
      " |  infrequent_categories_\n",
      " |      Infrequent categories for each feature.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |\n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |\n",
      " |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      " |      and returns a transformed version of `X`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input samples.\n",
      " |\n",
      " |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      " |          Target values (None for unsupervised transformations).\n",
      " |\n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |\n",
      " |  set_output(self, *, transform=None)\n",
      " |      Set output container.\n",
      " |\n",
      " |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      " |      for an example on how to use the API.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      transform : {\"default\", \"pandas\"}, default=None\n",
      " |          Configure output of `transform` and `fit_transform`.\n",
      " |\n",
      " |          - `\"default\"`: Default output format of a transformer\n",
      " |          - `\"pandas\"`: DataFrame output\n",
      " |          - `\"polars\"`: Polars output\n",
      " |          - `None`: Transform configuration is unchanged\n",
      " |\n",
      " |          .. versionadded:: 1.4\n",
      " |              `\"polars\"` option was added.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |\n",
      " |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs) from builtins.type\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |\n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |\n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |\n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |\n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __setstate__(self, state)\n",
      " |\n",
      " |  __sklearn_clone__(self)\n",
      " |\n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |\n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |\n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |\n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |\n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(OneHotEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this line of code, we create a `ColumnTransformer` object and pass it a list of data transformations. In this case, we only have a single transformation. This transform is defined using a transformer called `OneHotEncoder` from the `sklearn.preprocessing` library that is used to convert input variables to binary variables (one-hot encoding). In this case, we apply `OneHotEncoder` to the first column of the data (the column with index 0). The `'encoder'` in this line of code is just a name or identifier given to the transformation. It's a way to label the transformation being applied, in this case, the `OneHotEncoder()`.\n",
    "\n",
    "When you create a `ColumnTransformer`, you provide a list of transformations. Each transformation is a tuple that contains three elements:\n",
    "\n",
    "1. A name (string) for the transformation. This is `'encoder'` in our case.\n",
    "2. The transformer object. This is `OneHotEncoder()` in our case.\n",
    "3. The columns this transformation should be applied to. This is `[0]` in our case, meaning the transformation is applied to the first column of the input data.\n",
    "\n",
    "The `remainder='passthrough'` argument is used to specify how to handle columns that do not have a transform applied. In this case, the columns that do not have the transformation applied will be passed through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 0.0, 0.0, 44.0, 72000.0],\n",
       "       [0.0, 0.0, 1.0, 27.0, 48000.0],\n",
       "       [0.0, 1.0, 0.0, 30.0, 54000.0],\n",
       "       [0.0, 0.0, 1.0, 38.0, 61000.0],\n",
       "       [0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n",
       "       [1.0, 0.0, 0.0, 35.0, 58000.0],\n",
       "       [0.0, 0.0, 1.0, 38.77777777777778, 52000.0],\n",
       "       [1.0, 0.0, 0.0, 48.0, 79000.0],\n",
       "       [0.0, 1.0, 0.0, 50.0, 83000.0],\n",
       "       [1.0, 0.0, 0.0, 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitTransformX = ct.fit_transform(X) # fit and transform the data in X\n",
    "fitTransformX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 0.0, 0.0, 44.0, 72000.0],\n",
       "       [0.0, 0.0, 1.0, 27.0, 48000.0],\n",
       "       [0.0, 1.0, 0.0, 30.0, 54000.0],\n",
       "       [0.0, 0.0, 1.0, 38.0, 61000.0],\n",
       "       [0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n",
       "       [1.0, 0.0, 0.0, 35.0, 58000.0],\n",
       "       [0.0, 0.0, 1.0, 38.77777777777778, 52000.0],\n",
       "       [1.0, 0.0, 0.0, 48.0, 79000.0],\n",
       "       [0.0, 1.0, 0.0, 50.0, 83000.0],\n",
       "       [1.0, 0.0, 0.0, 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = fitTransformX\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Encoding the Dependent Variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LabelEncoder in module sklearn.preprocessing._label:\n",
      "\n",
      "class LabelEncoder(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  Encode target labels with value between 0 and n_classes-1.\n",
      " |\n",
      " |  This transformer should be used to encode target values, *i.e.* `y`, and\n",
      " |  not the input `X`.\n",
      " |\n",
      " |  Read more in the :ref:`User Guide <preprocessing_targets>`.\n",
      " |\n",
      " |  .. versionadded:: 0.12\n",
      " |\n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      Holds the label for each class.\n",
      " |\n",
      " |  See Also\n",
      " |  --------\n",
      " |  OrdinalEncoder : Encode categorical features using an ordinal encoding\n",
      " |      scheme.\n",
      " |  OneHotEncoder : Encode categorical features as a one-hot numeric array.\n",
      " |\n",
      " |  Examples\n",
      " |  --------\n",
      " |  `LabelEncoder` can be used to normalize labels.\n",
      " |\n",
      " |  >>> from sklearn.preprocessing import LabelEncoder\n",
      " |  >>> le = LabelEncoder()\n",
      " |  >>> le.fit([1, 2, 2, 6])\n",
      " |  LabelEncoder()\n",
      " |  >>> le.classes_\n",
      " |  array([1, 2, 6])\n",
      " |  >>> le.transform([1, 1, 2, 6])\n",
      " |  array([0, 0, 1, 2]...)\n",
      " |  >>> le.inverse_transform([0, 0, 1, 2])\n",
      " |  array([1, 1, 2, 6])\n",
      " |\n",
      " |  It can also be used to transform non-numerical labels (as long as they are\n",
      " |  hashable and comparable) to numerical labels.\n",
      " |\n",
      " |  >>> le = LabelEncoder()\n",
      " |  >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
      " |  LabelEncoder()\n",
      " |  >>> list(le.classes_)\n",
      " |  ['amsterdam', 'paris', 'tokyo']\n",
      " |  >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"])\n",
      " |  array([2, 2, 1]...)\n",
      " |  >>> list(le.inverse_transform([2, 2, 1]))\n",
      " |  ['tokyo', 'tokyo', 'paris']\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      LabelEncoder\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.utils._set_output._SetOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  fit(self, y)\n",
      " |      Fit label encoder.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self.\n",
      " |          Fitted label encoder.\n",
      " |\n",
      " |  fit_transform(self, y)\n",
      " |      Fit label encoder and return encoded labels.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Encoded labels.\n",
      " |\n",
      " |  inverse_transform(self, y)\n",
      " |      Transform labels back to original encoding.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          Target values.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          Original encoding.\n",
      " |\n",
      " |  transform(self, y)\n",
      " |      Transform labels to normalized encoding.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Labels as normalized encodings.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |\n",
      " |  set_output(self, *, transform=None)\n",
      " |      Set output container.\n",
      " |\n",
      " |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      " |      for an example on how to use the API.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      transform : {\"default\", \"pandas\"}, default=None\n",
      " |          Configure output of `transform` and `fit_transform`.\n",
      " |\n",
      " |          - `\"default\"`: Default output format of a transformer\n",
      " |          - `\"pandas\"`: DataFrame output\n",
      " |          - `\"polars\"`: Polars output\n",
      " |          - `None`: Transform configuration is unchanged\n",
      " |\n",
      " |          .. versionadded:: 1.4\n",
      " |              `\"polars\"` option was added.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |\n",
      " |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs) from builtins.type\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |\n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |\n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |\n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |\n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __setstate__(self, state)\n",
      " |\n",
      " |  __sklearn_clone__(self)\n",
      " |\n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |\n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |\n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |\n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |\n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LabelEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitTransformY = LabelEncoder().fit_transform(y)\n",
    "fitTransformY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = fitTransformY\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Splitting the dataset into the Training set and Test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encounter the lines of code above again, but this time we will carefully explain the concepts within it as follows:\n",
    "\n",
    "+ `test_size` is a parameter that determines the size of the test data set (test set) compared to the total number of original data samples. If `test_size` is a decimal number between 0.0 and 1.0, it represents a percentage of the test data. If `test_size` is an integer, it represents the absolute number of test data samples. If `test_size` is not specified (None), the default value will be the offset of the training set size, i.e. 0.25.\n",
    "\n",
    "+ `random_state` is the parameter that determines the random state when dividing the data into training and testing sets. If you want the data division result to remain the same across different runs, you can provide an integer for `random_state`. This ensures that each run will use the same random state, resulting in the same results. If you don't care about keeping the random state intact, you can leave `random_state` as None or not provide a value for it.\n",
    "\n",
    "For example, `test_size` is set to 0.2, so 20% of the original data will be used for the test set. `random_state` is set to 1, ensuring that the split result will be the same across different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 0.0, 1.0, 38.77777777777778, 52000.0],\n",
       "       [0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n",
       "       [1.0, 0.0, 0.0, 44.0, 72000.0],\n",
       "       [0.0, 0.0, 1.0, 38.0, 61000.0],\n",
       "       [0.0, 0.0, 1.0, 27.0, 48000.0],\n",
       "       [1.0, 0.0, 0.0, 48.0, 79000.0],\n",
       "       [0.0, 1.0, 0.0, 50.0, 83000.0],\n",
       "       [1.0, 0.0, 0.0, 35.0, 58000.0]], dtype=object)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 1.0, 0.0, 30.0, 54000.0],\n",
       "       [1.0, 0.0, 0.0, 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature Scaling.**\n",
    "\n",
    "Performing feature scaling before applying machine learning models is necessary because it ensures that all features contribute equally to the model's performance. As we move from simple linear regression to multivariate and polynomial, the number of features and their complexity increase. Without feature scaling, larger-scale features can disproportionately influence the model, leading to inaccurate predictions.\n",
    "\n",
    "+ Simple Linear Regression:\n",
    "\n",
    "$$y = b_0 + b_1x_1.$$\n",
    "\n",
    "+ Multiple Linear Regression:\n",
    "\n",
    "$$y = b_0 + b_1x_1 + b_2x_2 + \\ldots + b_nx_n.$$\n",
    "\n",
    "+ Polynomial Linear Regression:\n",
    "\n",
    "$$y = b_0 + b_1x_1 + b_2x_1^2 + \\ldots + b_nx_1^n.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, StandardScaler is: $X' = \\displaystyle\\frac{X - \\mu}{\\sigma}, \\ X'\\in[-3, 3]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
    "X_test[:, 3:] = sc.transform(X_test[:, 3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, you cannot use `StandardScaler()` directly but must first assign it to an object (in this case `sc`).\n",
    "\n",
    "`X_test` does not use the `sc.transform` function like `X_train` because when using `StandardScaler()`, we only need to fit and transform the training data (`X_train`) to calculate mean and standard deviation. Then, we use the calculated mean and standard deviation values from the training data to transform the test data (`X_test`) without re-fitting. This helps ensure that the scaling process is applied consistently to both training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 0.0, 1.0, -0.19159184384578545, -1.0781259408412425],\n",
       "       [0.0, 1.0, 0.0, -0.014117293757057777, -0.07013167641635372],\n",
       "       [1.0, 0.0, 0.0, 0.566708506533324, 0.633562432710455],\n",
       "       [0.0, 0.0, 1.0, -0.30453019390224867, -0.30786617274297867],\n",
       "       [0.0, 0.0, 1.0, -1.9018011447007988, -1.420463615551582],\n",
       "       [1.0, 0.0, 0.0, 1.1475343068237058, 1.232653363453549],\n",
       "       [0.0, 1.0, 0.0, 1.4379472069688968, 1.5749910381638885],\n",
       "       [1.0, 0.0, 0.0, -0.7401495441200351, -0.5646194287757332]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 1.0, 0.0, -1.4661817944830124, -0.9069571034860727],\n",
       "       [1.0, 0.0, 0.0, -0.44973664397484414, 0.2056403393225306]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
